<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning.">
  <meta name="keywords" content="CL-MVSNet">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png" type=image/png>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Kaiqiang Xiong<sup>1</sup>,</span>
            <span class="author-block">Rui Peng<sup>1</sup>,</span>
            <span class="author-block">Zhe Zhang<sup>1</sup>,</span>
            <span class="author-block">Tianxing Feng<sup>1</sup>,</span><br>
            <span class="author-block">
              <a href="https://jianbojiao.com/">Jianbo Jiao</a><sup>4</sup>,</span>
            <span class="author-block">Feng Gao<sup>5</sup>,</span>
            <span class="author-block">Ronggang Wang<sup>1,2,3</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Electronic and Computer Engineering, Peking University,</span><br>
            <span class="author-block"><sup>2</sup>Peng Cheng Laboratory,</span>
            <span class="author-block"><sup>3</sup>Migu Culture Technology Co., Ltd,</span><br>
            <span class="author-block"><sup>4</sup>School of Computer Science, University of Birmingham,</span>
            <span class="author-block"><sup>5</sup>School of Arts, Peking University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiong_CL-MVSNet_Unsupervised_Multi-View_Stereo_with_Dual-Level_Contrastive_Learning_ICCV_2023_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/KaiqiangXiong/CL-MVSNet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Unsupervised Multi-View Stereo (MVS) methods have achieved promising progress recently.
            However, previous methods primarily depend on the  photometric consistency assumption, which may suffer from two limitations: indistinguishable regions and view-dependent effects, e.g., low-textured areas and reflections. 
            To address these issues, in this paper, we propose a new dual-level contrastive learning approach, named CL-MVSNet. 
          </p>
          <p>
            Specifically, our model integrates two contrastive branches into an unsupervised MVS framework to construct additional supervisory signals. 
            On the one hand, we present an image-level contrastive branch to guide the model to acquire more context awareness, thus leading to more complete depth estimation in indistinguishable regions. 
            On the other hand, we exploit a scene-level contrastive branch to boost the representation ability, improving robustness to view-dependent effects. 
            Moreover, to recover more accurate 3D geometry, we introduce an L0.5 photometric consistency loss, which encourages the model to focus more on accurate points while mitigating the gradient penalty of undesirable ones.
          </p>
          <p>
            Extensive experiments on DTU and Tanks&Temples benchmarks demonstrate that our approach achieves state-of-the-art performance among all end-to-end unsupervised MVS frameworks and outperforms its supervised counterpart by a considerable margin without fine-tuning. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xiong2023cl,
  author    = {Xiong, Kaiqiang and Peng, Rui and Zhang, Zhe and Feng, Tianxing and Jiao, Jianbo and Gao, Feng and Wang, Ronggang},
  title     = {CL-MVSNet: Unsupervised Multi-View Stereo with Dual-Level Contrastive Learning},
  journal   = {ICCV},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/KaiqiangXiong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
